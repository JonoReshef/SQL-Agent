# WestBrand Email Analysis - Architectural Overview

## Executive Summary

A **full-stack, production-ready system** for analyzing Outlook emails to extract product information, match against inventory using **database-driven hierarchical filtering**, and generate comprehensive 5-sheet Excel reports with full database persistence. The system also provides a **natural language SQL chat interface** for querying the database. The system includes:

- **Email Analysis Backend**: Python-based email analysis using Azure OpenAI (gpt-5 with low reasoning effort) orchestrated via LangGraph workflows with fuzzy matching
- **SQL Chat Backend**: LangGraph-based natural language to SQL translation using Azure OpenAI (gpt-4.1) with conversation persistence via PostgreSQL checkpointer
- **REST API**: FastAPI server with Server-Sent Events (SSE) streaming for real-time chat responses
- **Frontend**: Modern Next.js 14 web interface with TypeScript and Tailwind CSS for interactive SQL chat
- **Infrastructure**: Docker Compose deployment for PostgreSQL 17 (with pgvector), Redis, backend, and frontend services

The system processes individual `.msg` files (no threading), uses PostgreSQL 17 with **thread_hash** as primary key for deduplication, and provides a **natural language SQL chat interface** with conversation persistence.

**Key Features**:

- Synchronous email analysis with LLM extraction
- PostgreSQL database with thread_hash PKs and content_hash for all records
- Database-driven hierarchical matching (10-100x faster than linear scan)
- Fuzzy property matching using rapidfuzz
- Multi-sheet Excel reports (3 or 5 sheets based on --match flag)
- **Natural language SQL chat interface with conversation persistence**
- **FastAPI REST API with Server-Sent Events streaming**
- **Next.js 14 frontend with real-time chat UI**
- **Full-stack Docker Compose deployment**
- Production-ready with health checks and monitoring

## Core Architectural Principles

### 1. Simplicity First

- **Synchronous Processing**: No async/await complexity
- **Single Email Analysis**: Each `.msg` file processed independently
- **No Thread Reconstruction**: Explicitly avoided - too complex and outside scope
- **Clear Data Flow**: Input â†’ Process â†’ Output

### 2. Test-Driven Development

- **Tests Before Code**: Write comprehensive tests first
- **Continuous Validation**: Run tests frequently during development
- **26/26 Tests Passing**: Strong foundation established
- **Pytest Framework**: Modern, well-documented testing

### 3. Type Safety

- **Pydantic v2**: All data structures typed and validated
- **Compile-time Checks**: Catch errors early
- **Self-documenting**: Models serve as API documentation

### 4. Database-Backed Persistence

- **PostgreSQL with pgvector**: Relational database for structured data
- **Foreign Key Relationships**: Proper data integrity
- **Upsert Operations**: Idempotent database writes
- **Optional Database**: System works without database if needed

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONFIGURATION LAYER                         â”‚
â”‚                   (products_config.yaml)                        â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Product Definitions (Fasteners, Threaded Rod, etc.)         â”‚
â”‚  â€¢ Properties to Extract (grade, size, material, etc.)         â”‚
â”‚  â€¢ Extraction Rules (patterns, formats)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATABASE LAYER                              â”‚
â”‚                  (PostgreSQL 17 + pgvector)                     â”‚
â”‚                                                                 â”‚
â”‚  Tables:                                                        â”‚
â”‚  â€¢ emails_processed                                            â”‚
â”‚    - thread_hash VARCHAR(64) PRIMARY KEY (SHA256 content hash) â”‚
â”‚    - file_path, subject, sender, date_sent                     â”‚
â”‚  â€¢ product_mentions                                            â”‚
â”‚    - id SERIAL PRIMARY KEY                                      â”‚
â”‚    - email_thread_hash FK â†’ emails_processed.thread_hash       â”‚
â”‚    - content_hash VARCHAR(64) (for change detection)           â”‚
â”‚  â€¢ inventory_items                                             â”‚
â”‚    - id SERIAL PRIMARY KEY                                      â”‚
â”‚    - item_number VARCHAR(100) UNIQUE                           â”‚
â”‚    - content_hash VARCHAR(64) (for change detection)           â”‚
â”‚  â€¢ inventory_matches                                           â”‚
â”‚    - id SERIAL PRIMARY KEY                                      â”‚
â”‚    - product_mention_id FK â†’ product_mentions.id              â”‚
â”‚    - inventory_item_id FK â†’ inventory_items.id                â”‚
â”‚    - content_hash VARCHAR(64) (for change detection)           â”‚
â”‚  â€¢ match_review_flags                                          â”‚
â”‚    - id SERIAL PRIMARY KEY                                      â”‚
â”‚    - product_mention_id FK â†’ product_mentions.id              â”‚
â”‚    - content_hash VARCHAR(64) (for change detection)           â”‚
â”‚  â€¢ checkpoints (LangGraph conversation persistence)            â”‚
â”‚    - thread_id, checkpoint_id, parent_checkpoint_id            â”‚
â”‚    - checkpoint (JSONB), metadata (JSONB)                      â”‚
â”‚                                                                 â”‚
â”‚  Features:                                                      â”‚
â”‚  â€¢ Foreign key constraints with CASCADE deletes                 â”‚
â”‚  â€¢ Indexes on all FKs and content_hash columns                  â”‚
â”‚  â€¢ Upsert operations based on natural keys                      â”‚
â”‚  â€¢ Content hashing for intelligent change detection             â”‚
â”‚  â€¢ Conversation state persistence for chat workflow             â”‚
â”‚  â€¢ Docker Compose for easy deployment                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“                    â†“                  â†“
              EMAIL ANALYSIS           SQL CHAT            WEB FRONTEND
                 WORKFLOW             WORKFLOW              (Next.js 14)
        (src/analysis_workflow/)  (src/chat_workflow/)   (frontend/)
                   â†“                      â†“                     â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  LangGraph  â”‚      â”‚  LangGraph   â”‚      â”‚  React 18   â”‚
            â”‚  Email      â”‚      â”‚  SQL Chat    â”‚      â”‚  TypeScript â”‚
            â”‚  Processor  â”‚      â”‚  Agent       â”‚      â”‚  Tailwind   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“                      â†“                     â†“
            5-Sheet Excel         FastAPI Server         SSE Streaming
                Report           (src/server/server.py)   Chat Interface
                                           â†‘                     â†“
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            REST API + SSE Events
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       INPUT PIPELINE                            â”‚
â”‚                                                                 â”‚
â”‚  Directory Scanner â†’ .msg File Discovery â†’ Email Parser         â”‚
â”‚                                                                 â”‚
â”‚  âœ… Implemented:                                                â”‚
â”‚    â€¢ read_msg_file(path) - Parse single email                  â”‚
â”‚    â€¢ read_msg_files_from_directory(dir) - Batch parse          â”‚
â”‚    â€¢ Extract metadata (subject, sender, recipients, date)      â”‚
â”‚    â€¢ Extract body (HTML, RTF, plain text)                      â”‚
â”‚    â€¢ List attachments                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PREPROCESSING PIPELINE                         â”‚
â”‚                                                                 â”‚
â”‚  HTML Stripping â†’ Signature Removal â†’ Text Cleaning            â”‚
â”‚                                                                 â”‚
â”‚  âœ… Implemented:                                                â”‚
â”‚    â€¢ strip_html_tags() - BeautifulSoup HTML parsing            â”‚
â”‚    â€¢ clean_signature() - Remove footers, quotes, separators    â”‚
â”‚    â€¢ Preserve core email content                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LANGGRAPH WORKFLOW (Synchronous)               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚INGESTION â”‚â†’ â”‚EXTRACTION â”‚â†’ â”‚MATCHING*â”‚â†’ â”‚PERSISTENCEâ”‚  â”‚
â”‚  â”‚   NODE   â”‚  â”‚   NODE    â”‚  â”‚  NODE   â”‚  â”‚   NODE     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    *conditional with --match   â”‚
â”‚                                           â†“                  â”‚
â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                                    â”‚ REPORTING â”‚       â”‚
â”‚                                    â”‚   NODE    â”‚       â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â”‚  Node Locations (src/analysis_workflow/nodes/):               â”‚
â”‚  â€¢ ingestion/ingestion.py - Load & parse .msg files          â”‚
â”‚  â€¢ extraction/extraction.py - LLM product extraction         â”‚
â”‚  â€¢ matching/matching.py - Hierarchical inventory matching    â”‚
â”‚    â””â”€ utils/hierarchy.py - Property hierarchies           â”‚
â”‚    â””â”€ utils/filtering.py - Database-driven filtering     â”‚
â”‚    â””â”€ utils/matcher.py - Match scoring & ranking         â”‚
â”‚    â””â”€ utils/normalizer.py - Property normalization       â”‚
â”‚  â€¢ persistence/persistence.py - Database storage with upsert â”‚
â”‚  â€¢ reporting/reporting.py - 5-sheet Excel generation         â”‚
â”‚                                                              â”‚
â”‚  State Machine (Pydantic BaseModel in src/models/workflow.py):â”‚
â”‚  {                                                            â”‚
â”‚    input_directory: str,                                      â”‚
â”‚    emails: List[Email] = [],                                  â”‚
â”‚    extracted_products: List[ProductMention] = [],             â”‚
â”‚    analytics: List[ProductAnalytics] = [],                    â”‚
â”‚    matching_enabled: bool = False,                            â”‚
â”‚    product_matches: List[InventoryMatch] = [],                â”‚
â”‚    review_flags: List[ReviewFlag] = [],                       â”‚
â”‚    report_path: str = "",                                       â”‚
â”‚    errors: List[str] = []  # Auto-initialized                 â”‚
â”‚  }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚INGESTION â”‚â†’ â”‚EXTRACTION â”‚â†’ â”‚MATCHING â”‚â†’ â”‚PERSISTENCE  â”‚â†’ â”‚REPORTING â”‚â”‚
â”‚  â”‚   NODE   â”‚  â”‚   NODE    â”‚  â”‚  NODE*  â”‚  â”‚   NODE      â”‚  â”‚  NODE    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                    *optional with --match flag             â”‚
â”‚                                                                 â”‚
â”‚  Ingestion:           Extraction:           Matching:           â”‚
â”‚  â€¢ Load .msg files    â€¢ Clean body text    â€¢ Load inventory    â”‚
â”‚  â€¢ Parse metadata     â€¢ LLM invoke()       â€¢ Fuzzy match props â”‚
â”‚  â€¢ Initial validation â€¢ Extract products   â€¢ Score confidence  â”‚
â”‚                       â€¢ Validate results    â€¢ Generate flags    â”‚
â”‚                                                                 â”‚
â”‚  Persistence:         Reporting:                                â”‚
â”‚  â€¢ Store emails       â€¢ Aggregate data                          â”‚
â”‚  â€¢ Store products     â€¢ Format tables                           â”‚
â”‚  â€¢ Store matches      â€¢ Add match/flag sheets                   â”‚
â”‚  â€¢ Store flags        â€¢ Generate Excel                          â”‚
â”‚                                                                 â”‚
â”‚  State Machine (Pydantic BaseModel):                            â”‚
â”‚  {                                                              â”‚
â”‚    input_directory: str,                                        â”‚
â”‚    emails: List[Email] = [],                                    â”‚
â”‚    extracted_products: List[ProductMention] = [],               â”‚
â”‚    analytics: List[ProductAnalytics] = [],                      â”‚
â”‚    inventory_items: List[InventoryItem] = [],                   â”‚
â”‚    product_matches: Dict[str, List[InventoryMatch]] = {},       â”‚
â”‚    review_flags: List[ReviewFlag] = [],                         â”‚
â”‚    report_path: str = "",                                       â”‚
â”‚    matching_enabled: bool = False,                              â”‚
â”‚    errors: List[str] = []  # Auto-initialized                   â”‚
â”‚  }                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 AI EXTRACTION LAYER                             â”‚
â”‚                  (Azure OpenAI gpt-4.1)                         â”‚
â”‚                                                                 â”‚
â”‚  Prompt Engineering:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ System: You are analyzing industrial product emails.    â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Extract:                                                 â”‚  â”‚
â”‚  â”‚ - Product names and categories                          â”‚  â”‚
â”‚  â”‚ - Properties (grade, size, material, finish, etc.)      â”‚  â”‚
â”‚  â”‚ - Quantities and units                                  â”‚  â”‚
â”‚  â”‚ - Context (quote request, order, inquiry)               â”‚  â”‚
â”‚  â”‚ - Dates mentioned                                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Return: JSON array of products                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  LLM Configuration:                                             â”‚
â”‚  â€¢ Deployment: gpt-4.1                                          â”‚
â”‚  â€¢ API Version: 2024-08-01-preview                              â”‚
â”‚  â€¢ Temperature: 0 (deterministic extraction)                    â”‚
â”‚  â€¢ Reasoning effort: low                                        â”‚
â”‚  â€¢ Method: llm.invoke() - synchronous                           â”‚
â”‚  â€¢ Caching: Redis (localhost:6379)                              â”‚
â”‚  â€¢ Response: Structured JSON via with_structured_output()       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 INVENTORY MATCHING LAYER                        â”‚
â”‚          (Database-Driven Hierarchical Filtering)               â”‚
â”‚                                                                 â”‚
â”‚  Module: src/analysis_workflow/nodes/matching/utils/            â”‚
â”‚                                                                 â”‚
â”‚  Property Hierarchy (hierarchy.py):                             â”‚
â”‚  â€¢ Loads from config/products_config.yaml                       â”‚
â”‚  â€¢ Cached with @lru_cache for performance                      â”‚
â”‚  â€¢ Defines filter order: grade â†’ size â†’ length â†’ material    â”‚
â”‚                                                                 â”‚
â”‚  Database Filtering (filtering.py):                             â”‚
â”‚  â€¢ Progressive SQL queries on inventory_items table            â”‚
â”‚  â€¢ Filters by category first (exact match)                     â”‚
â”‚  â€¢ Then by properties in hierarchy order                       â”‚
â”‚  â€¢ Uses JSON property column with GIN index                    â”‚
â”‚  â€¢ Stops when result set < 10 items (graceful degradation)    â”‚
â”‚  â€¢ 10-100x faster than in-memory linear scan                   â”‚
â”‚                                                                 â”‚
â”‚  Property Normalizer (normalizer.py):                           â”‚
â€¢  Normalize values: "Gr 8" â†’ "8", "ss" â†’ "stainless steel"    â”‚
â”‚  â€¢ Fuzzy match with rapidfuzz (80% similarity threshold)       â”‚
â”‚  â€¢ Batch normalization for performance                         â”‚
â”‚  â€¢ Handle common variations and typos                          â”‚
â”‚                                                                 â”‚
â”‚  Product Matcher (matcher.py):                                  â”‚
â”‚  â€¢ Calls database filtering to get candidates                  â”‚
â”‚  â€¢ Scores remaining items with weighted properties            â”‚
â”‚  â€¢ 40% name + 20% category + 40% properties                    â”‚
â”‚  â€¢ Ranks by score, returns top N matches                       â”‚
â”‚  â€¢ Generates match reasoning explanations                      â”‚
â”‚                                                                 â”‚
â”‚  Review Flag Generation:                                        â”‚
â”‚  â€¢ INSUFFICIENT_DATA - No matches found                        â”‚
â”‚  â€¢ LOW_CONFIDENCE - Score < 0.7                                â”‚
â”‚  â€¢ AMBIGUOUS_MATCH - Top 2 scores within 0.1                  â”‚
â”‚  â€¢ MISSING_PROPERTIES - â‰¥2 properties not in inventory         â”‚
â”‚  â€¢ Priority-coded actions for each flag type                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚ - Quantities and units                                  â”‚  â”‚
â”‚  â”‚ - Context (quote request, order, inquiry)               â”‚  â”‚
â”‚  â”‚ - Dates mentioned                                       â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚ Return: JSON array of products                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  LLM Configuration (Product Extraction):                        â”‚
â”‚  â€¢ Deployment: gpt-5                                            â”‚
â”‚  â€¢ Temperature: 0 (deterministic extraction)                    â”‚
â”‚  â€¢ Reasoning effort: low                                        â”‚
â”‚  â€¢ Method: llm.invoke() - synchronous                           â”‚
â”‚  â€¢ Response: Structured JSON via with_structured_output()       â”‚
â”‚                                                                 â”‚
â”‚  LLM Configuration (SQL Chat):                                  â”‚
â”‚  â€¢ Deployment: gpt-4.1                                          â”‚
â”‚  â€¢ Temperature: 0 (deterministic)                               â”‚
â”‚  â€¢ Method: llm.invoke() - synchronous                           â”‚
â”‚  â€¢ Response: Tool calls and messages                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OUTPUT GENERATION                            â”‚
â”‚                  (Excel Multi-Sheet Report)                     â”‚
â”‚                                                                 â”‚
â”‚  Sheet 1: PRODUCT MENTIONS (Detailed)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Product | Category | Properties | Quantity | Context  â”‚    â”‚
â”‚  â”‚ Date | Email Subject | Sender | Source File            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  Sheet 2: ANALYTICS (Aggregated)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Product | Total Mentions | Date Range | Total Quantity â”‚    â”‚
â”‚  â”‚ Property Variations | Contexts | Unique Senders         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  Sheet 3: EMAIL SUMMARY                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Email File | Subject | Sender | Date | Product Count  â”‚    â”‚
â”‚  â”‚ Has Attachments | Parse Status                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  Sheet 4: INVENTORY MATCHES (if --match enabled)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Product | Inventory Item # | Description | Match Scoreâ”‚    â”‚
â”‚  â”‚ Rank | Matched Props | Missing Props | Reasoning      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â€¢ Color-coded by score (green/yellow/orange)                  â”‚
â”‚  â€¢ NO MATCHES highlighted in red                                â”‚
â”‚                                                                 â”‚
â”‚  Sheet 5: REVIEW FLAGS (if --match enabled)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Product | Issue Type | Match Count | Top Confidence   â”‚    â”‚
â”‚  â”‚ Reason | Action Needed                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â€¢ Color-coded by priority (red/yellow/orange)                 â”‚
â”‚                                                                 â”‚
â”‚  Features:                                                      â”‚
â”‚  â€¢ Conditional formatting for easy reading                      â”‚
â”‚  â€¢ Frozen header rows                                           â”‚
â”‚  â€¢ Auto-filter columns                                          â”‚
â”‚  â€¢ Date formatting                                              â”‚
â”‚  â€¢ Pivot-table ready structure                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Models (Pydantic)

### Email Models

```python
EmailMetadata
  â€¢ message_id: Optional[str]
  â€¢ subject: str
  â€¢ sender: str
  â€¢ recipients: List[str]
  â€¢ cc: Optional[List[str]]
  â€¢ date: Optional[datetime]

Email
  â€¢ metadata: EmailMetadata
  â€¢ body: str
  â€¢ cleaned_body: Optional[str]
  â€¢ attachments: List[str]
  â€¢ file_path: Optional[str]
```

### Product Models

```python
ProductProperty
  â€¢ name: str (e.g., "grade", "size")
  â€¢ value: str
  â€¢ confidence: float (0.0 - 1.0)

ProductMention
  â€¢ exact_product_text: str  # Original text from email
  â€¢ product_name: str
  â€¢ product_category: str
  â€¢ properties: List[ProductProperty]
  â€¢ quantity: Optional[float]
  â€¢ unit: Optional[str]
  â€¢ context: str (quote_request, order, inquiry)
  â€¢ date_requested: Optional[str]
  â€¢ requestor: Optional[str]
  â€¢ email_subject: str
  â€¢ email_sender: str
  â€¢ email_file: Optional[str]

ProductAnalytics
  â€¢ product_name: str
  â€¢ product_category: str
  â€¢ total_mentions: int
  â€¢ first_mention: Optional[str]
  â€¢ last_mention: Optional[str]
  â€¢ total_quantity: Optional[float]
  â€¢ properties_summary: Dict[str, List[str]]
  â€¢ contexts: List[str]
  â€¢ people_involved: List[str]
```

### Inventory Models

```python
InventoryItem (extends ProductItem)
  â€¢ item_number: str  # Unique inventory ID
  â€¢ raw_description: str  # From Excel
  â€¢ exact_product_text: str
  â€¢ product_name: str
  â€¢ product_category: str
  â€¢ properties: List[ProductProperty]
  â€¢ parse_confidence: float
  â€¢ needs_manual_review: bool

InventoryMatch
  â€¢ inventory_item_number: str
  â€¢ inventory_description: str
  â€¢ match_score: float (0.0-1.0)
  â€¢ rank: int (1 = best match)
  â€¢ matched_properties: List[str]
  â€¢ missing_properties: List[str]
  â€¢ match_reasoning: str

ReviewFlag
  â€¢ product_text: str
  â€¢ product_name: str
  â€¢ product_category: str
  â€¢ issue_type: str  # INSUFFICIENT_DATA, LOW_CONFIDENCE, etc.
  â€¢ match_count: int
  â€¢ top_confidence: Optional[float]
  â€¢ reason: str
  â€¢ action_needed: str
```

### SQL Chat Models

```python
ChatState (LangGraph state)
  â€¢ messages: Annotated[List[BaseMessage], add]  # Conversation history
  â€¢ available_tables: List[str]  # Database table names
  â€¢ current_query: Optional[str]  # SQL being executed
  â€¢ query_result: Optional[str]  # Result from last query
  â€¢ error: Optional[str]  # Error messages
  â€¢ executed_queries: Annotated[List[QueryExecution], add]  # Query transparency
  â€¢ overall_summary: Optional[str]  # Search process summary

QueryExecution (for transparency)
  â€¢ query: str  # The actual SQL executed
  â€¢ query_explanation: QueryExplanation  # AI-generated explanation
  â€¢ raw_result: Optional[str]  # Query result

QueryExplanation
  â€¢ description: str  # One-line non-technical explanation
  â€¢ result_summary: str  # What was found (e.g., "Found 80 records")
```

## Technology Stack Justification

### Why These Libraries?

1. **PostgreSQL 17 + pgvector** (database)

   - âœ… Industry-standard relational database
   - âœ… pgvector extension for future semantic search
   - âœ… Strong ACID guarantees
   - âœ… Docker support for easy deployment
   - âœ… SQLAlchemy 2.0 compatibility

2. **SQLAlchemy 2.0** (ORM)

   - âœ… Modern async-capable ORM (using sync mode)
   - âœ… Type-safe queries with new syntax
   - âœ… Relationship management
   - âœ… Migration support via Alembic (future)
   - âœ… Connection pooling built-in

3. **rapidfuzz 3.14.3** (fuzzy matching)

   - âœ… Fast Levenshtein distance calculations
   - âœ… Multiple matching algorithms
   - âœ… Property normalization support
   - âœ… Active maintenance
   - âœ… Pure Python (no C deps)

4. **extract-msg** (vs mail-parser)

   - âœ… Already in requirements.txt
   - âœ… Specifically for Outlook .msg files
   - âœ… Handles RTF, HTML, plain text bodies
   - âœ… Extracts attachments and metadata
   - âŒ mail-parser: Not maintained since 2020, for standard email formats

5. **LangGraph** (vs raw LangChain)

   - âœ… State machine workflow management
   - âœ… Easy node composition
   - âœ… Built-in error handling
   - âœ… Synchronous execution support
   - âœ… Visual workflow representation
   - âœ… Redis caching integration

6. **BeautifulSoup4** (vs regex only)

   - âœ… Robust HTML parsing
   - âœ… Handles malformed HTML
   - âœ… Easy tag removal
   - âœ… Entity decoding
   - âœ… Preserves text content

7. **Pydantic v2** (vs dataclasses)

   - âœ… Runtime validation
   - âœ… JSON serialization
   - âœ… Type coercion
   - âœ… Documentation via models
   - âœ… OpenAPI integration ready

8. **openpyxl** (vs pandas/xlsxwriter)
   - âœ… Pure Python (no external deps)
   - âœ… Rich formatting support
   - âœ… Multiple sheet management
   - âœ… Formula support
   - âœ… Active maintenance

## Workflow Execution Flow

```
1. INITIALIZATION
   â”œâ”€ Load products_config.yaml
   â”œâ”€ Initialize Azure OpenAI client (with Redis caching)
   â”œâ”€ Test database connection (if --match flag)
   â””â”€ Create LangGraph workflow

2. INGESTION PHASE
   â”œâ”€ Scan directory for .msg files
   â”œâ”€ Parse each email (extract-msg)
   â”œâ”€ Validate Email models
   â””â”€ Add to workflow state

3. PREPROCESSING PHASE
   â”œâ”€ Strip HTML tags (BeautifulSoup)
   â”œâ”€ Remove signatures/footers
   â”œâ”€ Clean quoted text
   â””â”€ Update Email.cleaned_body

4. EXTRACTION PHASE (per email)
   â”œâ”€ Build LLM prompt with:
   â”‚  â”œâ”€ Product definitions from config
   â”‚  â”œâ”€ Cleaned email body
   â”‚  â””â”€ Example extractions
   â”œâ”€ Call llm.invoke() - synchronous
   â”œâ”€ Parse JSON response via structured_output
   â”œâ”€ Validate ProductMention models
   â””â”€ Add to state.extracted_products

5. MATCHING PHASE (if --match enabled)
   â”œâ”€ Load inventory items from database
   â”œâ”€ For each extracted product:
   â”‚  â”œâ”€ Filter by category (exact match)
   â”‚  â”œâ”€ Normalize properties (rapidfuzz)
   â”‚  â”œâ”€ Calculate Jaccard similarity scores
   â”‚  â”œâ”€ Rank matches by score
   â”‚  â””â”€ Generate review flags if needed
   â””â”€ Update state with matches and flags

6. PERSISTENCE PHASE
   â”œâ”€ Store emails to emails_processed table
   â”œâ”€ Store products to product_mentions table (FK to emails)
   â”œâ”€ If matching enabled:
   â”‚  â”œâ”€ Store matches to inventory_matches (FK to products & inventory)
   â”‚  â””â”€ Store flags to match_review_flags (FK to products)
   â””â”€ Commit transactions

7. ANALYTICS PHASE
   â”œâ”€ Group products by name/category
   â”œâ”€ Calculate aggregates:
   â”‚  â”œâ”€ Total mentions
   â”‚  â”œâ”€ Date ranges
   â”‚  â”œâ”€ Quantity sums
   â”‚  â””â”€ Property variations
   â””â”€ Create ProductAnalytics models

8. REPORTING PHASE
   â”œâ”€ Create Excel workbook (openpyxl)
   â”œâ”€ Generate Sheet 1: Product Mentions
   â”œâ”€ Generate Sheet 2: Analytics
   â”œâ”€ Generate Sheet 3: Email Summary
   â”œâ”€ If matching enabled:
   â”‚  â”œâ”€ Generate Sheet 4: Inventory Matches (color-coded)
   â”‚  â””â”€ Generate Sheet 5: Review Flags (priority-coded)
   â”œâ”€ Apply formatting and filters
   â””â”€ Save to output directory

9. ERROR HANDLING
   â”œâ”€ Log parsing errors (continue processing)
   â”œâ”€ Record LLM failures
   â”œâ”€ Track validation failures
   â”œâ”€ Database transaction rollbacks
   â””â”€ Generate error report in state
```

## Key Design Decisions Explained

### 1. No Email Threading

**Rationale**: Email thread reconstruction is complex and error-prone:

- Unreliable headers (In-Reply-To, References often missing)
- Subject line matching (RE:, FW: variations)
- Date/time alignment across timezones
- Out of scope for current requirements

**Solution**: Treat each email independently, extract all product mentions per email.

### 2. Synchronous Processing

**Rationale**:

- Simpler debugging and error tracking
- Easier to reason about execution flow
- No concurrency bugs
- Performance adequate for current scale (~10-20 emails/min)

**Trade-off**: Could process faster with async, but added complexity not worth it yet.

### 3. Test-Driven Development

**Rationale**:

- Catch bugs early in email parsing (complex format handling)
- Document expected behavior
- Enable confident refactoring
- Ensure LLM extraction reliability

**Current Coverage**: 26 passing tests across email parsing and signature cleaning.

### 4. Configurable Products

**Rationale**:

- Business requirements change frequently
- Different customers need different product types
- Properties vary by industry
- No code changes needed for new products

**Implementation**: YAML config with product definitions, aliases, and properties.

## Error Handling Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ERROR CATEGORIES                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚ 1. PARSING ERRORS                                        â”‚
â”‚    â€¢ Corrupted .msg file â†’ Skip file, log error         â”‚
â”‚    â€¢ Missing required fields â†’ Use defaults             â”‚
â”‚    â€¢ Encoding issues â†’ Fallback encoding                â”‚
â”‚                                                          â”‚
â”‚ 2. LLM ERRORS                                            â”‚
â”‚    â€¢ API timeout â†’ Retry 3x with backoff                â”‚
â”‚    â€¢ Invalid JSON â†’ Log, continue with empty result     â”‚
â”‚    â€¢ Rate limiting â†’ Sleep and retry                    â”‚
â”‚                                                          â”‚
â”‚ 3. VALIDATION ERRORS                                     â”‚
â”‚    â€¢ Pydantic validation fail â†’ Log details             â”‚
â”‚    â€¢ Missing required fields â†’ Skip product             â”‚
â”‚    â€¢ Invalid dates â†’ Use None                           â”‚
â”‚                                                          â”‚
â”‚ 4. REPORT GENERATION ERRORS                              â”‚
â”‚    â€¢ Excel write fail â†’ Raise exception (fatal)         â”‚
â”‚    â€¢ Permission denied â†’ Clear error message            â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Characteristics

**Current Benchmarks** (estimated):

- Email parsing: ~100ms per file
- Signature cleaning: ~50ms per email
- LLM extraction: ~2-5 seconds per email (Azure OpenAI latency)
- Excel generation: ~500ms for 100 rows

**Bottleneck**: Azure OpenAI API calls (synchronous)

**Scalability Path** (future):

1. Implement LLM response caching
2. Batch similar emails together
3. Switch to async if needed
4. Consider local LLM for faster processing

## Security Considerations

1. **API Keys**: Stored in `.env` file (gitignored)
2. **Email Content**: Processed locally, not sent to third parties (except Azure OpenAI)
3. **PII Handling**: Email addresses extracted but not filtered
4. **File Permissions**: Excel reports created with user permissions

## Maintenance & Monitoring

**Logging Strategy**:

- Info: Each phase completion
- Warning: Parsing failures (continue processing)
- Error: LLM failures, validation errors
- Debug: Detailed extraction results

**Metrics to Track**:

- Emails processed / failed
- Products extracted per email
- LLM call success rate
- Average processing time

## SQL Chat Workflow Architecture

### Overview

The **SQL Chat Workflow** (`src/chat_workflow/`) provides a natural language interface to query the WestBrand PostgreSQL database. Users can ask questions in plain English then the system enriches the queries using Azure OpenAI gpt-4.1 with prior conversational content then the system translates these to SQL queries using gpt-5. The system features **question enrichment** to better understand user intent, **query transparency** with AI-generated explanations, and **conversation persistence** for multi-turn interactions.

### Key Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SQL CHAT WORKFLOW                             â”‚
â”‚                  (LangGraph State Machine)                      â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Enrich   â”‚â†’ â”‚ Generate  â”‚â†’ â”‚  Execute    â”‚â†’ â”‚ Generate â”‚  â”‚
â”‚  â”‚ Question â”‚  â”‚  Query    â”‚  â”‚   Query     â”‚  â”‚ Explain. â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â†‘               â†“                         â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                    (Loop for follow-ups)                        â”‚
â”‚                                                                 â”‚
â”‚  State: ChatState (Pydantic v2 - src/models/chat_models.py)   â”‚
â”‚  â€¢ user_question: Current question being answered              â”‚
â”‚  â€¢ enriched_query: QuestionEnrichment (additional context)     â”‚
â”‚  â€¢ messages: Conversation history (add reducer)                â”‚
â”‚  â€¢ query_result: AIMessage with last LLM response              â”‚
â”‚  â€¢ executed_queries: List[QueryExecution] (transparency)       â”‚
â”‚  â€¢ overall_summary: AI-generated summary of search process     â”‚
â”‚  â€¢ available_tables: Discovered table names                    â”‚
â”‚  â€¢ current_query: Last SQL query executed                      â”‚
â”‚  â€¢ execute_result: Result from last query                      â”‚
â”‚  â€¢ error: Error message if operation failed                    â”‚
â”‚                                                                 â”‚
â”‚  Persistence: PostgreSQL Checkpointer (LangGraph)              â”‚
â”‚  â€¢ Thread-based conversation history                           â”‚
â”‚  â€¢ Survives server restarts                                    â”‚
â”‚  â€¢ Enables multi-turn conversations                            â”‚
â”‚  â€¢ Checkpoint tables: checkpoints, checkpoint_writes           â”‚
â”‚                                                                 â”‚
â”‚  Caching: Redis (LangChain cache)                              â”‚
â”‚  â€¢ Caches LLM responses to reduce redundant calls              â”‚
â”‚  â€¢ Shared across conversation threads                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Node Responsibilities

1. **enrich_question** (`nodes/enrich_question.py`):

   - Takes user's original question
   - Uses LLM to expand into 1-3 detailed sub-questions
   - Provides context about user's intent and goals
   - Returns `QuestionEnrichment` with additional questions and intended goal

2. **generate_query** (`nodes/generate_query.py`):

   - Receives enriched question and conversation history
   - LLM with tool binding (`run_query_tool`, `get_schema_tool`)
   - Converts natural language to PostgreSQL queries
   - Can call tools multiple times to gather schema info
   - Returns AIMessage with tool_calls or final answer

3. **execute_query** (`nodes/execute_query.py`):

   - Extracts SQL from tool_calls in AIMessage
   - Validates queries are SELECT only (security)
   - Executes against PostgreSQL database
   - Creates `QueryExecution` objects with query and raw result
   - Appends to state.executed_queries for transparency
   - Returns ToolMessage with results

4. **generate_explanations** (`nodes/generate_explanations.py`):
   - Processes all queries in state.executed_queries
   - Generates AI explanations in parallel using ThreadPoolExecutor
   - Creates one-line non-technical descriptions
   - Generates result summaries ("Found 80 records")
   - Produces overall summary of entire search process
   - Updates QueryExecution objects with QueryExplanation

### Workflow Execution Flow

```
1. USER INPUT
   â”œâ”€ Natural language question (e.g., "How many emails were processed?")
   â”œâ”€ Thread ID for conversation continuity
   â””â”€ Submit via CLI or REST API

2. ENRICH QUESTION NODE
   â”œâ”€ Receives user question and conversation history
   â”œâ”€ LLM generates 1-3 clarifying sub-questions
   â”œâ”€ Examples:
   â”‚  User: "What were sales last quarter?"
   â”‚  Enriched:
   â”‚    1. Total sales figures for each month in last quarter
   â”‚    2. Breakdown by product category
   â”‚    3. Significant trends or anomalies
   â”œâ”€ Creates QuestionEnrichment object:
   â”‚  {
   â”‚    additional_questions: List[str],
   â”‚    intended_goal: Optional[str]
   â”‚  }
   â””â”€ Adds HumanMessage to state with enriched context

3. GENERATE QUERY NODE
   â”œâ”€ Receives:
   â”‚  â”œâ”€ Original user question
   â”‚  â”œâ”€ Enriched questions and goals
   â”‚  â”œâ”€ Conversation history (all previous messages)
   â”‚  â”œâ”€ Previously executed queries and results
   â”‚  â””â”€ WestBrand system prompt with domain knowledge
   â”œâ”€ LLM bound with tools:
   â”‚  â”œâ”€ run_query_tool: Execute PostgreSQL SELECT queries
   â”‚  â””â”€ get_schema_tool: Fetch table schemas dynamically
   â”œâ”€ LLM decides:
   â”‚  â”œâ”€ Generate new SQL queries if more data needed
   â”‚  â”œâ”€ Call get_schema_tool if schema info needed
   â”‚  â””â”€ Provide final answer if question fully answered
   â”œâ”€ Returns AIMessage with:
   â”‚  â”œâ”€ tool_calls (if more queries needed) OR
   â”‚  â””â”€ content (final answer text)
   â””â”€ Updates state.query_result

4. CONDITIONAL ROUTING (should_continue)
   â”œâ”€ Checks state.query_result.tool_calls
   â”œâ”€ If tool_calls exist â†’ go to EXECUTE QUERY
   â””â”€ If no tool_calls â†’ go to GENERATE EXPLANATIONS (done)

5. EXECUTE QUERY NODE (if tool calls present)
   â”œâ”€ Iterates through all tool_calls
   â”œâ”€ For each run_query_tool call:
   â”‚  â”œâ”€ Extract SQL query from tool arguments
   â”‚  â”œâ”€ Validate: Must be SELECT only (security)
   â”‚  â”œâ”€ Execute query against PostgreSQL
   â”‚  â”œâ”€ Create QueryExecution object:
   â”‚  â”‚  {
   â”‚  â”‚    query: str (the SQL),
   â”‚  â”‚    raw_result: str (database result),
   â”‚  â”‚    query_explanation: None (filled later)
   â”‚  â”‚  }
   â”‚  â””â”€ Append to state.executed_queries
   â”œâ”€ Creates ToolMessage for each result
   â”œâ”€ Updates state.current_query and execute_result
   â””â”€ Loops back to GENERATE QUERY for next step

6. GENERATE EXPLANATIONS NODE (when done querying)
   â”œâ”€ Receives all QueryExecution objects from state
   â”œâ”€ For each query (parallel with ThreadPoolExecutor):
   â”‚  â”œâ”€ Sends query + result to LLM
   â”‚  â”œâ”€ LLM generates QueryExplanation:
   â”‚  â”‚  {
   â”‚  â”‚    description: "One-line non-technical explanation",
   â”‚  â”‚    result_summary: "Found 80 records" or similar
   â”‚  â”‚  }
   â”‚  â””â”€ Adds explanation to QueryExecution
   â”œâ”€ Generates overall_summary:
   â”‚  â””â”€ AI summary of entire multi-query search process
   â”œâ”€ Updates state with explained queries
   â””â”€ Workflow ends (returns to user)

7. PERSISTENCE (automatic at each step)
   â”œâ”€ LangGraph PostgresSaver checkpoints every state change
   â”œâ”€ Checkpoint includes:
   â”‚  â”œâ”€ thread_id (conversation identifier)
   â”‚  â”œâ”€ checkpoint_id (unique per state)
   â”‚  â”œâ”€ Full message history
   â”‚  â”œâ”€ All executed queries with explanations
   â”‚  â””â”€ Metadata (step number, timestamp)
   â””â”€ Enables conversation resume and history retrieval

8. FINAL RESPONSE TO USER
   â”œâ”€ Natural language answer from LLM
   â”œâ”€ All executed queries with:
   â”‚  â”œâ”€ ğŸ’¡ One-line explanation
   â”‚  â”œâ”€ ğŸ“ˆ Result summary
   â”‚  â””â”€ Formatted SQL query
   â””â”€ Overall summary of search process
```

### Data Models

#### State Models (`src/models/chat_models.py`)

```python
class QuestionEnrichment(BaseModel):
    """Enrichment details for user question"""
    additional_questions: List[str]  # Clarifying sub-questions
    intended_goal: Optional[str]     # Why these questions help

class QueryExplanation(BaseModel):
    """AI-generated explanation of query execution"""
    description: str                 # One-line non-technical explanation
    result_summary: str | None       # "Found 80 records" or similar

class QueryExecution(BaseModel):
    """Single SQL query execution with transparency"""
    query: str                       # The actual SQL
    raw_result: Optional[str]        # Database result
    query_explanation: Optional[QueryExplanation]  # AI explanation

class ChatState(BaseModel):
    """LangGraph state for SQL chat workflow"""
    user_question: str                              # Current question
    enriched_query: QuestionEnrichment              # Expanded context
    messages: Annotated[List[BaseMessage], add]     # Conversation history (add reducer)
    query_result: AIMessage                         # Last LLM response
    executed_queries: List[QueryExecution]          # All queries (transparency)
    overall_summary: Optional[str]                  # AI summary of search
    available_tables: List[str]                     # Discovered tables
    current_query: Optional[str]                    # Last SQL executed
    execute_result: Optional[str]                   # Last query result
    error: Optional[str]                            # Error if any
```

**Key Features:**

- Pydantic v2 with `ConfigDict(arbitrary_types_allowed=True)` for LangChain types
- `messages` uses `add` reducer (never overwrites, always appends)
- All queries tracked in `executed_queries` for full transparency
- AI-generated explanations added post-execution

### API Interfaces

#### 1. CLI Interface (`cli.py`)

```bash
python -m src.chat_workflow.cli

# Interactive REPL with question enrichment and query transparency
You: How many emails are in the system?

Enriching question...
ğŸ¤– Agent: There are 156 emails in the database.

======================================================================
ğŸ“Š SQL Queries Executed:
======================================================================

Query 1:
  ğŸ’¡ Counts the total number of processed email records
  ğŸ“ˆ Result: Found 156 records

  SQL:
    SELECT COUNT(*) AS email_count FROM emails_processed;

Overall Summary:
Retrieved total email count by querying emails_processed table.
======================================================================
```

#### 2. REST API (`api.py` - FastAPI)

**Status**: API implementation exists but may require updates to match current workflow (4-node design with enrichment and explanations).

**Non-Streaming Endpoint:**

```bash
POST /chat
{
  "message": "How many emails are in the system?",
  "thread_id": "user-123"
}

Response:
{
  "response": "There are 156 emails in the database.",
  "executed_queries": [
    {
      "query": "SELECT COUNT(*) FROM emails_processed;",
      "query_explanation": {
        "description": "Counts total processed emails",
        "result_summary": "Found 156 records"
      },
      "raw_result": "[(156,)]"
    }
  ],
  "overall_summary": "Retrieved email count from database."
}
```

**Streaming Endpoint:**

```bash
POST /chat/stream
Server-Sent Events (SSE):
data: {"type": "token", "content": "There"}
data: {"type": "token", "content": " are"}
data: {"type": "sql", "query": "SELECT COUNT(*)..."}
data: {"type": "end"}
```

data: {"type": "end"}

````

### Key Features

#### Query Transparency

**Every query execution is tracked with AI-generated explanations for full transparency.**

**Benefits**:
- Users understand what SQL is being run
- Educational - learn SQL by example
- Debugging - verify query correctness
- Audit trail - track database access
- Multi-query workflow visibility

**Implementation** (`QueryExecution` model):

```python
QueryExecution
  â€¢ query: str  # Actual SQL executed
  â€¢ query_explanation: QueryExplanation
    - description: str  # "Counts total emails in database"
    - result_summary: str  # "Found 156 records"
  â€¢ raw_result: Optional[str]  # Database result: "[(156,)]"
````

**Display Format**:

```
Query 1:
  ğŸ’¡ Counts the total number of processed email records
  ğŸ“ˆ Result: Found 156 records

  SQL:
    SELECT COUNT(*) FROM emails_processed;
```

#### Question Enrichment

**LLM expands user questions into detailed sub-questions** to better understand intent.

**Example**:

```
User: "What were sales last quarter?"

Enriched Questions:
1. Total sales figures for each month in last quarter
2. Breakdown by product category
3. Significant trends or anomalies

Intended Goal: Provide comprehensive quarterly sales analysis
```

**Implementation** (`QuestionEnrichment` model):

```python
QuestionEnrichment
  â€¢ additional_questions: List[str]  # 1-3 clarifying questions
  â€¢ intended_goal: Optional[str]     # Why these help
```

#### Conversation Persistence

**PostgreSQL checkpointer** stores full conversation history:

- Thread ID links related conversations
- Survives server restarts
- Enables conversation resume
- Full message history with state

**Tables** (auto-created by LangGraph):

- `checkpoints`: Main checkpoint storage
- `checkpoint_writes`: Intermediate writes

#### Safety Features

**Read-Only SQL Enforcement**:

- Only SELECT queries allowed
- INSERT, UPDATE, DELETE rejected
- DDL operations (DROP, ALTER, CREATE) blocked
- Query validation before execution

### Security Features

1. **Read-Only Access**: Only SELECT queries allowed (validated with regex)
2. **SQL Injection Protection**: Uses psycopg parameterized queries
3. **Error Handling**: Database errors caught and returned as error messages
4. **No DDL/DML**: CREATE, DROP, INSERT, UPDATE, DELETE all blocked

### Conversation Persistence

**PostgreSQL Checkpointer** (`langgraph-checkpoint-postgres`):

- Stores conversation state in `checkpoints` table
- Each message adds a new checkpoint
- Thread ID links related checkpoints
- Enables conversation history retrieval
- Survives server restarts

**Checkpoint Schema**:

```sql
CREATE TABLE checkpoints (
    thread_id TEXT,
    checkpoint_id TEXT PRIMARY KEY,
    parent_checkpoint_id TEXT,
    checkpoint JSONB,  -- Full state snapshot
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Testing

**Test Coverage** (52/56 tests passing - 93%):

- `test_graph.py`: Workflow state machine tests (3 tests need updates)
- `test_api.py`: FastAPI endpoint tests
- `test_execute_query.py`: SQL execution tests
- `test_list_tables.py`: Table discovery tests
- `test_models.py`: Pydantic model validation tests
- `test_sql_transparency.py`: Query transparency tests (1 test needs update)
- `test_db_wrapper.py`: Database wrapper tests

### Domain Knowledge (System Prompts)

**Custom prompts** (`prompts.py`) include:

- WestBrand database schema context
- Common query patterns (email counts, product mentions, etc.)
- Property extraction logic
- Inventory matching concepts
- Best practices for SQL generation

**Example prompt snippet**:

```
You are a SQL expert helping users query the WestBrand database.

Available tables:
- emails_processed: Email metadata (subject, sender, date)
- product_mentions: Extracted products from emails
- inventory_items: Available inventory with properties
- inventory_matches: Product-to-inventory matches
- match_review_flags: Quality issues with matches

Always use SELECT queries only. Never modify data.
Provide clear explanations for every query.
```

## FastAPI REST API Architecture (`src/server/server.py`)

### Overview

The **unified REST API** provides HTTP endpoints for both email analysis and SQL chat functionality, with **Server-Sent Events (SSE)** streaming support for real-time responses. This single server handles all backend API requests from the frontend and other clients.

### Key Features

1. **Unified Endpoint**: Single server for all API functionality
2. **Server-Sent Events Streaming**: Real-time response streaming
3. **Non-Streaming Fallback**: JSON responses for compatibility
4. **CORS Middleware**: Frontend integration support
5. **Conversation Persistence**: Thread-based history via PostgreSQL
6. **Query Transparency**: All SQL queries with explanations
7. **Health Monitoring**: Health check endpoint
8. **Anticipate Complexity**: Toggle analysis depth

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FASTAPI SERVER                              â”‚
â”‚                  (src/server/server.py)                         â”‚
â”‚                                                                 â”‚
â”‚  Middleware:                                                    â”‚
â”‚  â€¢ CORSMiddleware (allow_origins=["*"] - dev mode)            â”‚
â”‚                                                                 â”‚
â”‚  Endpoints:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GET /                                                     â”‚ â”‚
â”‚  â”‚   Root with API information                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ POST /chat/stream (PRIMARY)                              â”‚ â”‚
â”‚  â”‚   â€¢ Server-Sent Events streaming                         â”‚ â”‚
â”‚  â”‚   â€¢ Real-time status updates                             â”‚ â”‚
â”‚  â”‚   â€¢ Token-by-token response streaming                    â”‚ â”‚
â”‚  â”‚   â€¢ Query transparency with explanations                 â”‚ â”‚
â”‚  â”‚   â€¢ Overall summary at completion                        â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ Request: ChatRequest (Pydantic)                          â”‚ â”‚
â”‚  â”‚   â€¢ message: str (user question)                         â”‚ â”‚
â”‚  â”‚   â€¢ thread_id: str (conversation continuity)             â”‚ â”‚
â”‚  â”‚   â€¢ anticipate_complexity: bool (analysis depth)         â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ Response: StreamingResponse (text/event-stream)          â”‚ â”‚
â”‚  â”‚   Event types:                                           â”‚ â”‚
â”‚  â”‚   â€¢ status: Processing updates                           â”‚ â”‚
â”‚  â”‚   â€¢ message: AI response content                         â”‚ â”‚
â”‚  â”‚   â€¢ queries: SQL with explanations/summaries             â”‚ â”‚
â”‚  â”‚   â€¢ summary: Overall workflow summary                    â”‚ â”‚
â”‚  â”‚   â€¢ end: Stream completion                               â”‚ â”‚
â”‚  â”‚   â€¢ error: Error messages                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ POST /chat (FALLBACK)                                    â”‚ â”‚
â”‚  â”‚   â€¢ Non-streaming JSON response                          â”‚ â”‚
â”‚  â”‚   â€¢ Complete response in single payload                  â”‚ â”‚
â”‚  â”‚   â€¢ Same request model as streaming                      â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ Response: ChatResponse (Pydantic)                        â”‚ â”‚
â”‚  â”‚   â€¢ response: str (full answer)                          â”‚ â”‚
â”‚  â”‚   â€¢ thread_id: str (echo back)                           â”‚ â”‚
â”‚  â”‚   â€¢ executed_queries: List[QueryExecutionResponse]       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GET /history/{thread_id}                                 â”‚ â”‚
â”‚  â”‚   â€¢ Retrieves conversation history                       â”‚ â”‚
â”‚  â”‚   â€¢ All checkpoints with messages                        â”‚ â”‚
â”‚  â”‚   â€¢ Metadata and timestamps                              â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ Response: HistoryResponse (Pydantic)                     â”‚ â”‚
â”‚  â”‚   â€¢ thread_id: str                                       â”‚ â”‚
â”‚  â”‚   â€¢ history: List[CheckpointData]                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GET /health                                              â”‚ â”‚
â”‚  â”‚   â€¢ Health check for monitoring                          â”‚ â”‚
â”‚  â”‚   â€¢ Returns {"status": "healthy"}                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  Graph Integration:                                             â”‚
â”‚  â€¢ Singleton pattern for graph instance                        â”‚
â”‚  â€¢ get_graph() function caches LangGraph workflow              â”‚
â”‚  â€¢ Synchronous execution in async wrapper                      â”‚
â”‚  â€¢ PostgreSQL checkpointer for state persistence               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request/Response Models (`src/models/server.py`)

```python
class ChatRequest(BaseModel):
    message: str  # User's question
    thread_id: str  # Conversation thread ID
    anticipate_complexity: bool = False  # Analysis depth toggle

class QueryExecutionResponse(BaseModel):
    query: str  # Executed SQL
    explanation: str  # Human-readable description
    result_summary: str  # Brief result summary

class ChatResponse(BaseModel):
    response: str  # Agent's answer
    thread_id: str  # Thread ID
    executed_queries: list[QueryExecutionResponse]  # Query transparency

class MessageHistory(BaseModel):
    type: str  # Message type (HumanMessage, AIMessage)
    content: str  # Message content

class CheckpointData(BaseModel):
    checkpoint_id: str
    messages: list[MessageHistory]
    timestamp: Optional[str]
    metadata: dict[str, Any]

class HistoryResponse(BaseModel):
    thread_id: str
    history: list[CheckpointData]
```

### Anticipate Complexity Feature

Controls analysis depth and thoroughness:

- **`false` (default)**: Direct answers with minimal queries

  - Skips question enrichment
  - Max 10 query iterations
  - Faster execution
  - Best for straightforward questions

- **`true`**: Thorough exploratory analysis
  - Performs question enrichment (1-3 sub-questions)
  - Max 30 query iterations
  - Comprehensive results
  - Best for complex, ambiguous questions

### Server-Sent Events (SSE) Implementation

**Streaming Workflow**:

1. Client opens SSE connection to `/chat/stream`
2. Server streams events as workflow progresses:
   - Status updates during processing
   - Final AI message when complete
   - All executed queries with explanations
   - Overall summary of workflow
   - End event to close connection
3. Client processes events in real-time
4. Connection closes after end event

**Event Format** (JSON in `data:` field):

```javascript
data: {"type": "status", "content": "Executing query..."}
data: {"type": "message", "content": "The database contains 156 emails."}
data: {"type": "queries", "queries": [...]}
data: {"type": "summary", "content": "Retrieved email count"}
data: {"type": "end"}
```

### Running the Server

```bash
# Development with auto-reload
uvicorn src.server.server:app --reload --host 0.0.0.0 --port 8000

# Production via Docker Compose
docker-compose up -d backend

# Direct execution
python -m src.server.server
```

### CORS Configuration

**Current**: `allow_origins=["*"]` (development mode)

**Production**: Must restrict to specific frontend domains:

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

## Next.js Frontend Architecture (`frontend/`)

### Overview

The **frontend** is a modern web application built with **Next.js 14**, **React 18**, and **TypeScript**, providing a real-time streaming chat interface for the WestBrand SQL Chat Agent. It uses **Server-Sent Events** for live response streaming and **Tailwind CSS** for responsive design.

### Key Features

1. **Real-time Streaming**: Token-by-token response display via SSE
2. **Multi-Thread Management**: Create, switch, delete conversation threads
3. **SQL Transparency**: Syntax-highlighted queries with copy-to-clipboard
4. **Responsive Design**: Mobile-first with collapsible sidebar
5. **Local Storage Persistence**: Conversations saved in browser
6. **Type Safety**: Auto-generated types from backend OpenAPI schema
7. **Error Handling**: Graceful connection management

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NEXT.JS 14 FRONTEND                         â”‚
â”‚                      (React 18 + TypeScript)                    â”‚
â”‚                                                                 â”‚
â”‚  App Router (app/):                                             â”‚
â”‚  â”œâ”€â”€ layout.tsx          Root layout with metadata             â”‚
â”‚  â”œâ”€â”€ page.tsx            Home (redirects to /chat)             â”‚
â”‚  â”œâ”€â”€ globals.css         Tailwind CSS + global styles          â”‚
â”‚  â””â”€â”€ chat/                                                      â”‚
â”‚      â””â”€â”€ page.tsx        Main chat page                        â”‚
â”‚                                                                 â”‚
â”‚  Component Hierarchy:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ChatInterface (Container)                                â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ ChatSidebar                                          â”‚ â”‚
â”‚  â”‚ â”‚   â”œâ”€â”€ New Thread Button                                â”‚ â”‚
â”‚  â”‚ â”‚   â””â”€â”€ ThreadItem[]                                     â”‚ â”‚
â”‚  â”‚ â”‚       â””â”€â”€ Delete Button (with confirmation)            â”‚ â”‚
â”‚  â”‚ â”œâ”€â”€ ChatMessages                                         â”‚ â”‚
â”‚  â”‚ â”‚   â”œâ”€â”€ Message[] (User/Assistant)                       â”‚ â”‚
â”‚  â”‚ â”‚   â”‚   â”œâ”€â”€ Markdown rendering                           â”‚ â”‚
â”‚  â”‚ â”‚   â”‚   â””â”€â”€ QueryDisplay (SQL transparency)              â”‚ â”‚
â”‚  â”‚ â”‚   â”‚       â”œâ”€â”€ Syntax highlighted SQL                   â”‚ â”‚
â”‚  â”‚ â”‚   â”‚       â”œâ”€â”€ Explanation text                         â”‚ â”‚
â”‚  â”‚ â”‚   â”‚       â”œâ”€â”€ Result summary                           â”‚ â”‚
â”‚  â”‚ â”‚   â”‚       â””â”€â”€ Copy to clipboard button                 â”‚ â”‚
â”‚  â”‚ â”‚   â””â”€â”€ StreamingIndicator (loading state)               â”‚ â”‚
â”‚  â”‚ â””â”€â”€ ChatInput                                            â”‚ â”‚
â”‚  â”‚     â”œâ”€â”€ Auto-resizing textarea                           â”‚ â”‚
â”‚  â”‚     â””â”€â”€ Send button                                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  Custom Hooks (hooks/):                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ useChatStream                                            â”‚ â”‚
â”‚  â”‚   â€¢ Manages SSE connection to backend                    â”‚ â”‚
â”‚  â”‚   â€¢ Handles event types (status, message, queries, etc.)â”‚ â”‚
â”‚  â”‚   â€¢ Automatic reconnection on errors                     â”‚ â”‚
â”‚  â”‚   â€¢ Cleanup on unmount                                   â”‚ â”‚
â”‚  â”‚   â€¢ Loading state management                             â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ Returns:                                                 â”‚ â”‚
â”‚  â”‚   â€¢ messages: Message[]                                  â”‚ â”‚
â”‚  â”‚   â€¢ isStreaming: boolean                                 â”‚ â”‚
â”‚  â”‚   â€¢ error: string | null                                 â”‚ â”‚
â”‚  â”‚   â€¢ sendMessage(text: string): void                      â”‚ â”‚
â”‚  â”‚   â€¢ stopStreaming(): void                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ useChatThreads                                           â”‚ â”‚
â”‚  â”‚   â€¢ Thread state management                              â”‚ â”‚
â”‚  â”‚   â€¢ localStorage persistence                             â”‚ â”‚
â”‚  â”‚   â€¢ Auto-save on changes                                 â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ Returns:                                                 â”‚ â”‚
â”‚  â”‚   â€¢ threads: Thread[]                                    â”‚ â”‚
â”‚  â”‚   â€¢ activeThreadId: string                               â”‚ â”‚
â”‚  â”‚   â€¢ createThread(): void                                 â”‚ â”‚
â”‚  â”‚   â€¢ switchThread(id: string): void                       â”‚ â”‚
â”‚  â”‚   â€¢ deleteThread(id: string): void                       â”‚ â”‚
â”‚  â”‚   â€¢ updateThreadName(id, name): void                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ useLocalStorage                                          â”‚ â”‚
â”‚  â”‚   â€¢ Generic localStorage wrapper                         â”‚ â”‚
â”‚  â”‚   â€¢ TypeScript support                                   â”‚ â”‚
â”‚  â”‚   â€¢ Auto-serialization                                   â”‚ â”‚
â”‚  â”‚                                                           â”‚ â”‚
â”‚  â”‚ const [value, setValue] = useLocalStorage<T>(key, init) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  Type Generation (scripts/):                                    â”‚
â”‚  â€¢ generate-types.cjs - OpenAPI to TypeScript                  â”‚
â”‚  â€¢ npm run sync-types - Auto-generate from backend             â”‚
â”‚  â€¢ Output: types/server/server-types.ts                        â”‚
â”‚                                                                 â”‚
â”‚  Libraries:                                                     â”‚
â”‚  â€¢ next: ^14.0.4                                                â”‚
â”‚  â€¢ react: ^18.2.0                                               â”‚
â”‚  â€¢ react-markdown: ^10.1.0 (markdown rendering)                â”‚
â”‚  â€¢ react-syntax-highlighter: ^15.5.0 (SQL highlighting)        â”‚
â”‚  â€¢ tailwindcss: ^3.4.0 (styling)                                â”‚
â”‚  â€¢ openapi-typescript: ^6.7.3 (type generation)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. USER INTERACTION
   User types message â†’ Send button click
   â†“
2. FRONTEND STATE UPDATE
   useChatStream.sendMessage() called
   â†“
3. SSE CONNECTION OPENED
   EventSource connects to /chat/stream
   Request: {message, thread_id, anticipate_complexity}
   â†“
4. BACKEND PROCESSING
   LangGraph workflow executes
   â†“
5. REAL-TIME STREAMING
   Events streamed back:
   â€¢ status: "Executing query..."
   â€¢ message: "The database contains..."
   â€¢ queries: [{"query": "SELECT...", "explanation": "..."}]
   â€¢ summary: "Retrieved email count..."
   â€¢ end: (close stream)
   â†“
6. FRONTEND RENDERING
   â€¢ Streaming indicator shows during processing
   â€¢ Messages appear in chat as received
   â€¢ SQL queries rendered with syntax highlighting
   â€¢ Copy buttons enabled for SQL queries
   â€¢ Conversation saved to localStorage
```

### Environment Configuration

`frontend/.env.local`:

```env
NEXT_PUBLIC_API_URL=http://localhost:8000
```

### Running the Frontend

```bash
# Development mode
cd frontend
npm install
npm run dev  # http://localhost:3000

# Production build
npm run build
npm start

# Docker (via docker-compose)
docker-compose up -d frontend
```

### Deployment (Docker)

**Multi-stage Dockerfile**:

1. **Build stage**: npm install + next build
2. **Production stage**: node + next start
3. **Exposed port**: 3000

**Included in `docker-compose.yml`** as `westbrand-frontend` service:

```yaml
frontend:
  build:
    context: ./frontend
    dockerfile: Dockerfile
  container_name: westbrand-frontend
  environment:
    NEXT_PUBLIC_API_URL: http://localhost:8000
  ports:
    - '3000:3000'
  depends_on:
    - backend
  networks:
    - westbrand-network
  restart: unless-stopped
```

### Type Safety

**Auto-generated types from backend OpenAPI schema**:

```bash
# Generate types
npm run sync-types

# Uses openapi-typescript package
# Reads from: http://localhost:8000/openapi.json
# Writes to: types/server/server-types.ts
```

**Ensures frontend/backend type consistency**.

### Browser Support

- Chrome/Edge (latest)
- Firefox (latest)
- Safari (latest)
- Mobile: iOS Safari, Chrome Mobile

### Performance Optimizations

1. **Code Splitting**: Next.js automatic route-based splitting
2. **Lazy Loading**: Components loaded on demand
3. **Local Storage**: Reduce backend calls for thread management
4. **SSE Streaming**: Incremental rendering for better perceived performance
5. **React 18 Concurrent Features**: Automatic batching and transitions

## Docker Compose Deployment Architecture

### Overview

The complete system is deployed using **Docker Compose** with 4 services: PostgreSQL, Redis, Backend, and Frontend. This provides a production-ready environment with health checks, volume persistence, and proper networking.

### Services

```yaml
services:
  pgdb: # PostgreSQL 17 with pgvector
    image: pgvector/pgvector:pg17
    container_name: westbrand-db
    ports: ['5432:5432']
    volumes: [pgdata:/var/lib/postgresql/data]
    healthcheck: pg_isready checks
    restart: unless-stopped

  redis: # Redis Stack Server
    image: redis/redis-stack-server:latest
    container_name: westbrand-redis
    ports: ['6379:6379']
    volumes: [redis_data:/data]
    healthcheck: redis-cli ping
    restart: unless-stopped

  backend: # Python FastAPI Server
    build: .
    container_name: westbrand-backend
    ports: ['8000:8000']
    environment:
      - AZURE_LLM_API_KEY
      - AZURE_LLM_ENDPOINT
      - DATABASE_URL=postgresql://${PGUSER}:${PGPASSWORD}@pgdb:5432/${PGDATABASE}
      - REDIS_URL=redis://redis:6379
    depends_on:
      redis: { condition: service_healthy }
      pgdb: { condition: service_healthy }
    volumes:
      - ./data:/app/data
      - ./output:/app/output
      - ./config:/app/config
    restart: unless-stopped

  frontend: # Next.js 14 Web UI
    build: ./frontend
    container_name: westbrand-frontend
    ports: ['3000:3000']
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on: [backend]
    restart: unless-stopped

volumes:
  redis_data:
  pgdata:

networks:
  westbrand-network:
    driver: bridge
```

### Deployment Commands

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Check service status
docker-compose ps

# Stop all services
docker-compose down

# Rebuild and restart
docker-compose up -d --build
```

### Service URLs

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **PostgreSQL**: localhost:5432
- **Redis**: localhost:6379

### Health Checks

All services include health checks for reliable startup:

- **PostgreSQL**: `pg_isready` command
- **Redis**: `redis-cli ping` command
- **Backend**: Depends on healthy database and cache
- **Frontend**: Depends on healthy backend

### Volume Persistence

- **`pgdata`**: PostgreSQL database files (persistent)
- **`redis_data`**: Redis cache files (persistent)
- **`./data`**: Email .msg files (bind mount)
- **`./output`**: Generated Excel reports (bind mount)
- **`./config`**: Configuration files (bind mount)

### Network Isolation

All services communicate via **`westbrand-network`** bridge network:

- Inter-service communication uses service names (e.g., `pgdb`, `redis`)
- Isolated from other Docker networks
- Exposed ports for external access

## Future Enhancements (Not in Scope)

1. âŒ Email thread reconstruction (explicitly avoided)
2. âŒ Async processing (not needed yet)
3. âŒ Real-time processing (batch workflow)
4. âœ… **Natural language database interface** (SQL Chat implemented)
5. âœ… **Database storage** (PostgreSQL implemented)
6. âŒ Multi-language support (English only)
7. âŒ Semantic search with pgvector (prepared but not implemented)
8. âœ… **Chat workflow web UI** (Next.js frontend implemented)
9. âœ… **FastAPI REST API with streaming** (Server-Sent Events implemented)
10. âœ… **Full-stack Docker Compose deployment** (4-service architecture implemented)
11. ğŸ”„ Query result caching
12. ğŸ”„ Multi-database support
13. ğŸ”„ User authentication and authorization
14. ğŸ”„ Dark mode for frontend
15. ğŸ”„ Export chat history to PDF/markdown

## Deployment Requirements

**System Requirements**:

- Python 3.11+
- 4GB RAM minimum (for database + LLM caching)
- 500MB disk space
- Internet connection (Azure OpenAI)
- Docker & Docker Compose (for PostgreSQL)

**Infrastructure**:

- PostgreSQL 17 with pgvector extension
- Redis 7 for LLM response caching
- Docker containers for both services

**Dependencies**: See `requirements.txt` and `pyproject.toml`

Key libraries:

- extract-msg==0.55.0
- beautifulsoup4==4.13.5
- langgraph==1.0.3
- langgraph-checkpoint-postgres==2.0.13
- langchain-openai==1.0.2
- langchain-redis==0.1.6
- pydantic==2.12.4
- openpyxl==3.1.5
- sqlalchemy==2.0.36
- psycopg[binary]==3.2.12
- rapidfuzz==3.14.3
- fastapi==0.115.5
- uvicorn[standard]==0.34.0
- pytest==9.0.1
- pyyaml==6.0.3

## Success Metrics

**Phase 1: Foundation** âœ… COMPLETE

- [x] Comprehensive test coverage (24 test files)
- [x] Email parsing working
- [x] Signature cleaning implemented
- [x] Pydantic models defined

**Phase 2: Core Workflow** âœ… COMPLETE

- [x] LangGraph workflow implemented
- [x] Azure OpenAI integration working
- [x] Product extraction functional
- [x] Configuration system complete

**Phase 3: Database & Matching** âœ… COMPLETE

- [x] PostgreSQL database schema
- [x] SQLAlchemy models and operations
- [x] Inventory loader and parser
- [x] Fuzzy property matching
- [x] Product-to-inventory matching
- [x] Review flag generation

**Phase 4: Production Ready** âœ… COMPLETE

- [x] Excel report generation (5 sheets)
- [x] Integration tests passing (8/8)
- [x] End-to-end workflow complete
- [x] Documentation finalized
- [x] Database persistence working

**Phase 5: Deployment** âœ… COMPLETE

- [x] Docker Compose configuration
- [x] Database migration scripts
- [x] Import scripts for inventory
- [x] SQL Chat Workflow with FastAPI
- [x] Natural language database queries
- [x] Conversation persistence via PostgreSQL checkpointer
- [x] Query transparency with AI explanations
- [x] Comprehensive test suite operational
- [ ] Full inventory import (11,197 items)
- [ ] Production deployment guide

---

**Document Version**: 3.1  
**Last Updated**: November 26, 2025  
**Status**: Production Ready - Full-Stack System Operational
